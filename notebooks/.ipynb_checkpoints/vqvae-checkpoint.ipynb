{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ead33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b56b3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from models.backbone import build_backbone\n",
    "from models.pointalign import SmallDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2b7fc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    '''\n",
    "    Represents the VQ-VAE layer.\n",
    "    Implements the algorithm in 'Generating Diverse High Fidelity Images with VQ-VAE2' by Razavi et al.\n",
    "    https://arxiv.org/pdf/1906.00446\n",
    "    \n",
    "    Adapted from: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py,\n",
    "                  https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
    "\n",
    "    Attributes:\n",
    "        num_embed: int, the number of codebook vectors in the latent space\n",
    "        embed_dim: int, the dimensionality of the tensors in the latent space\n",
    "        commitment_cost: scalar that controls weighting of commitment loss term (beta in Eq 4 of the VQ-VAE 2 paper)\n",
    "        decay: float, decay for the moving averages\n",
    "        eps: small float constant to avoid numerical instability\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_embed, embed_dim, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embed = num_embed\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(embed_dim, num_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embed))\n",
    "        self.register_buffer(\"dw\", embed.clone())\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x - Tensor of shape (N, C, H, embed_dim)\n",
    "        '''\n",
    "        x_flat = x.reshape(-1, self.embed_dim) # (NCH, embed_dim)\n",
    "        \n",
    "        dists = (x_flat**2).sum(dim=1, keepdim=True) - 2 * (x_flat @ self.embed) + (self.embed**2).sum(dim=0, keepdim=True) #(NCH, num_embed)\n",
    "        \n",
    "        encoding_inds = dists.argmin(dim=-1) # (NCH,)\n",
    "        encodings = F.one_hot(encoding_inds, self.num_embed).type(x_flat.dtype) # (NCH, num_embed)\n",
    "        assert x.shape[-1:][0] == self.embed_dim\n",
    "        encoding_inds = encoding_inds.view(*x.shape[:-1]) # (N, C, H)\n",
    "        quantized = self.quantize(encoding_inds) # (N, C, H, embed_dim)\n",
    "        \n",
    "        if self.training:\n",
    "            '''\n",
    "            encodings_onehot_sum = encodings.sum(0)\n",
    "            encodings_sum = x_flat.transpose(0,1) @ encodings\n",
    "            self.cluster_size.data = self.cluster_size * self.decay + (1 - self.decay) * encodings_onehot_sum #N^(t)\n",
    "            self.dw.data = self.decay * self.dw.data + (1-self.decay) * encodings_sum\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = n * (self.cluster_size + self.eps)/(n + self.num_embed * self.eps)\n",
    "            embed_normalized = self.dw / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "            '''\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self.cluster_size.data)\n",
    "            cluster_size = n * (self.cluster_size + self.eps)/(n + self.num_embed * self.eps)\n",
    "            \n",
    "            #m^(t) = self.decay * m^(t-1) + (1-self.decay) * \\sum_{j}^{n^(t)} E(x)_{i,j}^(t)\n",
    "            self.dw = self.decay * self.dw + (1 - self.decay) * x_flat.T @ encodings\n",
    "            \n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = n * (self.cluster_size + self.eps) / (n + self.eps * self.num_embed)\n",
    "            self.embed = self.dw / cluster_size.unsqueeze(0) #e^(t) = m^(t) / N^(t)\n",
    "            \n",
    "        #else:\n",
    "            #loss = self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        diff = ((quantized.detach() - x)**2).mean()\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, diff, encoding_inds\n",
    "    \n",
    "\n",
    "    def quantize(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8f4c6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    '''\n",
    "    Represents the VQ-VAE layer with an exponential moving average loss to accelerate training.\n",
    "    Implements the algorithm in 'Generating Diverse High Fidelity Images with VQ-VAE2' by Razavi et al.\n",
    "    https://arxiv.org/pdf/1906.00446\n",
    "    \n",
    "    Adapted from: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py,\n",
    "                  https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py.\n",
    "\n",
    "    Attributes:\n",
    "        num_embed: int, the number of codebook vectors in the latent space\n",
    "        embed_dim: int, the dimensionality of the tensors in the latent space\n",
    "        commitment_cost: scalar that controls weighting of commitment loss term (beta in Eq 4 of the VQ-VAE 2 paper)\n",
    "        decay: float, decay for the moving averages\n",
    "        eps: small float constant to avoid numerical instability\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_embed, embed_dim, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embed = num_embed\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(embed_dim, num_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(num_embed))\n",
    "        self.register_buffer(\"dw\", embed.clone())\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x - Tensor of shape (N, C, H, embed_dim)\n",
    "        \n",
    "        '''\n",
    "        x_flat = x.reshape(-1, self.embed_dim) # (NCH, embed_dim)\n",
    "        \n",
    "        dists = (x_flat**2).sum(dim=1, keepdim=True) - 2 * (x_flat @ self.embed) + (self.embed**2).sum(dim=0, keepdim=True) #(NCH, num_embed)\n",
    "        \n",
    "        encoding_inds = dists.argmin(dim=-1) # (NCH,)\n",
    "        encodings = F.one_hot(encoding_inds, self.num_embed).type(x_flat.dtype) # (NCH, num_embed)\n",
    "        assert x.shape[-1:][0] == self.embed_dim\n",
    "        encoding_inds = encoding_inds.view(*x.shape[:-1]) # (N, C, H)\n",
    "        quantized = self.quantize(encoding_inds) # (N, C, H, embed_dim)\n",
    "        \n",
    "        if self.training:\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self.cluster_size.data)\n",
    "            cluster_size = n * (self.cluster_size + self.eps)/(n + self.num_embed * self.eps)\n",
    "            \n",
    "            #m^(t) = self.decay * m^(t-1) + (1-self.decay) * \\sum_{j}^{n^(t)} E(x)_{i,j}^(t)\n",
    "            self.dw = self.decay * self.dw + (1 - self.decay) * x_flat.T @ encodings\n",
    "            \n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = n * (self.cluster_size + self.eps) / (n + self.eps * self.num_embed)\n",
    "            self.embed = self.dw / cluster_size.unsqueeze(0) #e^(t) = m^(t) / N^(t)\n",
    "            \n",
    "        #else:\n",
    "            #loss = self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        diff = ((quantized.detach() - x)**2).mean()\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, diff, encoding_inds\n",
    "    \n",
    "\n",
    "    def quantize(self, embed_ind):\n",
    "        return F.embedding(embed_ind, self.embed.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89c7473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 64, 64])\n",
      "torch.Size([])\n",
      "torch.Size([128, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "num_embed = 512\n",
    "embed_dim = 64\n",
    "x = torch.rand(128, 3, 64, 64)\n",
    "\n",
    "vqvae = VectorQuantizer(num_embed, embed_dim, decay=0.99, eps=1e-5)\n",
    "quantized, diff, encoding_inds = vqvae.forward(x)\n",
    "print(quantized.shape)\n",
    "print(diff.shape)\n",
    "print(encoding_inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b8294700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, points=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder, feat_dims = build_backbone('resnet18', pretrained=True)\n",
    "        self.quantize = VectorQuantizer(num_embed=512, embed_dim=64)\n",
    "        self.decoder = SmallDecoder(points, feat_dims[-1], 128)\n",
    "\n",
    "    def forward(self, images, P=None):\n",
    "        z = self.encoder(images)[-1]\n",
    "\n",
    "        quant, diff, encoding_id = self.quantize(z.permute(0, 2, 3, 1))\n",
    "        quant = quant.permute(0, 3, 1, 2)\n",
    "        diff = diff.unsqueeze(0)\n",
    "\n",
    "        ptclds = self.decoder(quant, P)\n",
    "\n",
    "        return ptclds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d17e4989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/carolinechoi/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8368ddb3b94f4d8b2189b52deb0ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VQVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97b189ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 2, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([4, 3, 2, 1, 0])\n",
    "print(x)\n",
    "F.one_hot(x, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "15d57973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_inds = torch.tensor([[1,0,0],[0,1,0]])\n",
    "print(encoding_inds)\n",
    "F.one_hot(encoding_inds, 6).type(encoding_inds.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70fa3a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = torch.zeros(3, 6, device=encoding_inds.device)\n",
    "encodings.scatter_(1, encoding_inds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74ad74f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = torch.zeros(3, 6, device=encoding_inds.device)\n",
    "encodings.scatter_(1, encoding_inds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c4d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
